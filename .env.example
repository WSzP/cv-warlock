# CV Warlock Environment Configuration
# edit and save this file as .env.local
# never commit your .env.local file to version control!

# Required: At least one LLM provider API key
OPENAI_API_KEY=sk-your-openai-key-here
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here
GOOGLE_API_KEY=AI-your-google-key-here-for-gemini

# Optional: Default provider and model
CV_WARLOCK_PROVIDER=anthropic
CV_WARLOCK_MODEL=claude-opus-4-5-20251101

# Optional: Logging level (DEBUG, INFO, WARNING, ERROR)
CV_WARLOCK_LOG_LEVEL=INFO

# Optional: Langsmith for detailed logging and tracing
# get your free Langsmith API key at https://langchain.com/langsmith
LANGSMITH_API_KEY=lsv2_pt_your-langsmith-key-here
LANGSMITH_TRACING=true
# if you are in the EU region, use the EU endpoint https://eu.api.smith.langchain.com
LANGSMITH_ENDPOINT=https://eu.api.smith.langchain.com
# you can give a different name to your langsmith project if you want
LANGSMITH_PROJECT=cv-warlock

# RLM (Recursive Language Model) Configuration
# RLM enables handling of arbitrarily long CVs and job specs through
# code-based context exploration and sub-model calls (Dual-Model Strategy)
CV_WARLOCK_RLM_ENABLED=true
# Character count threshold to trigger RLM mode (default: 8000, use 1000 for testing RLMs)
CV_WARLOCK_RLM_SIZE_THRESHOLD=1000
# Maximum orchestrator iterations per analysis (default: 20)
CV_WARLOCK_RLM_MAX_ITERATIONS=20
# Maximum sub-LLM calls per analysis (default: 15)
CV_WARLOCK_RLM_MAX_SUB_CALLS=15
# Total timeout for RLM analysis in seconds (default: 300)
CV_WARLOCK_RLM_TIMEOUT_SECONDS=300
# Sandbox mode for code execution: local, docker, modal (default: local)
CV_WARLOCK_RLM_SANDBOX_MODE=local

# PDF Import Configuration
# Exclude work experiences that ended before this year (default: 2012)
# Set to 0 to include all experiences regardless of end date
CV_WARLOCK_EXPERIENCE_CUTOFF_YEAR=2012